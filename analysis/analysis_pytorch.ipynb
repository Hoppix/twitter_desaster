{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a classifier pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDisasterClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
    "        super(TweetDisasterClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "        \n",
    "        self.fc_last = nn.Linear(hidden_sizes[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            out = hidden_layer(out)\n",
    "            out = self.relu(out)\n",
    "        \n",
    "        out = self.fc_last(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 74 # dimensionality for maximum of 30 words\n",
    "hidden_size = [74, 264, 128]\n",
    "num_classes = 2\n",
    "\n",
    "model = TweetDisasterClassifier(input_size, hidden_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "df = df.sample(frac=1)\n",
    "df['tokenized_text'] = df['text'].apply(tokenizer) # todo maybe try with n-grams\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>4401</td>\n",
       "      <td>electrocute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@devon_breneman hopefully it doesn't electrocu...</td>\n",
       "      <td>0</td>\n",
       "      <td>[@devon_breneman, hopefully, it, doesn, ', t, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452</th>\n",
       "      <td>4936</td>\n",
       "      <td>exploded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reasons @BlueWestlo has exploded on @YouTube #...</td>\n",
       "      <td>0</td>\n",
       "      <td>[reasons, @bluewestlo, has, exploded, on, @you...</td>\n",
       "      <td>[11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4536</th>\n",
       "      <td>6450</td>\n",
       "      <td>injured</td>\n",
       "      <td>Tropical SE FLorida</td>\n",
       "      <td>#WakeUpFlorida... #Floridians more likely to b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[#wakeupflorida, ., ., ., #floridians, more, l...</td>\n",
       "      <td>[22, 20, 20, 20, 23, 24, 25, 26, 27, 28, 29, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4941</th>\n",
       "      <td>7043</td>\n",
       "      <td>mayhem</td>\n",
       "      <td>Detroit, Michigan</td>\n",
       "      <td>I liked a @YouTube video from @itsjustinstuart...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, liked, a, @youtube, video, from, @itsjusti...</td>\n",
       "      <td>[38, 39, 30, 16, 40, 41, 42, 43, 19, 20, 44, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>1870</td>\n",
       "      <td>burned</td>\n",
       "      <td>Cherry Creek Denver CO</td>\n",
       "      <td>Metal Cutting Sparks Brush Fire In Brighton: A...</td>\n",
       "      <td>1</td>\n",
       "      <td>[metal, cutting, sparks, brush, fire, in, brig...</td>\n",
       "      <td>[48, 49, 50, 51, 52, 53, 54, 30, 51, 52, 55, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6705</th>\n",
       "      <td>9605</td>\n",
       "      <td>thunder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thunder???</td>\n",
       "      <td>0</td>\n",
       "      <td>[thunder, ?, ?, ?]</td>\n",
       "      <td>[1416, 87, 87, 87]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5386</th>\n",
       "      <td>7686</td>\n",
       "      <td>panic</td>\n",
       "      <td>Milwaukee WI</td>\n",
       "      <td>Someone asked me about a monkey fist about 2 f...</td>\n",
       "      <td>0</td>\n",
       "      <td>[someone, asked, me, about, a, monkey, fist, a...</td>\n",
       "      <td>[2431, 4239, 617, 211, 30, 13081, 9137, 211, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>2984</td>\n",
       "      <td>dead</td>\n",
       "      <td>ÌÏT: -26.695807,27.837865</td>\n",
       "      <td>@kg4vaal lmaov.v hard the 'Ny' is the the new ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[@kg4vaal, lmaov, ., v, hard, the, ', ny, ', i...</td>\n",
       "      <td>[23534, 23535, 20, 6234, 2363, 126, 4, 7306, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6649</th>\n",
       "      <td>9525</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>MAD as Hell</td>\n",
       "      <td>RT AbbsWinston: #Zionist #Terrorist Demolish T...</td>\n",
       "      <td>1</td>\n",
       "      <td>[rt, abbswinston, #zionist, #terrorist, demoli...</td>\n",
       "      <td>[1208, 1471, 1472, 1473, 988, 23539, 8444, 161...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6251</th>\n",
       "      <td>8933</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>Mountains</td>\n",
       "      <td>New #photo Oak in a snowstorm http://t.co/JhSC...</td>\n",
       "      <td>0</td>\n",
       "      <td>[new, #photo, oak, in, a, snowstorm, http, //t...</td>\n",
       "      <td>[119, 5005, 5006, 53, 30, 2131, 43, 19, 20, 23...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword                   location  \\\n",
       "3067  4401  electrocute                        NaN   \n",
       "3452  4936     exploded                        NaN   \n",
       "4536  6450      injured        Tropical SE FLorida   \n",
       "4941  7043       mayhem          Detroit, Michigan   \n",
       "1295  1870       burned     Cherry Creek Denver CO   \n",
       "...    ...          ...                        ...   \n",
       "6705  9605      thunder                        NaN   \n",
       "5386  7686        panic               Milwaukee WI   \n",
       "2078  2984         dead  ÌÏT: -26.695807,27.837865   \n",
       "6649  9525    terrorist                MAD as Hell   \n",
       "6251  8933    snowstorm                  Mountains   \n",
       "\n",
       "                                                   text  target  \\\n",
       "3067  @devon_breneman hopefully it doesn't electrocu...       0   \n",
       "3452  Reasons @BlueWestlo has exploded on @YouTube #...       0   \n",
       "4536  #WakeUpFlorida... #Floridians more likely to b...       0   \n",
       "4941  I liked a @YouTube video from @itsjustinstuart...       0   \n",
       "1295  Metal Cutting Sparks Brush Fire In Brighton: A...       1   \n",
       "...                                                 ...     ...   \n",
       "6705                                         Thunder???       0   \n",
       "5386  Someone asked me about a monkey fist about 2 f...       0   \n",
       "2078  @kg4vaal lmaov.v hard the 'Ny' is the the new ...       0   \n",
       "6649  RT AbbsWinston: #Zionist #Terrorist Demolish T...       1   \n",
       "6251  New #photo Oak in a snowstorm http://t.co/JhSC...       0   \n",
       "\n",
       "                                         tokenized_text  \\\n",
       "3067  [@devon_breneman, hopefully, it, doesn, ', t, ...   \n",
       "3452  [reasons, @bluewestlo, has, exploded, on, @you...   \n",
       "4536  [#wakeupflorida, ., ., ., #floridians, more, l...   \n",
       "4941  [i, liked, a, @youtube, video, from, @itsjusti...   \n",
       "1295  [metal, cutting, sparks, brush, fire, in, brig...   \n",
       "...                                                 ...   \n",
       "6705                                 [thunder, ?, ?, ?]   \n",
       "5386  [someone, asked, me, about, a, monkey, fist, a...   \n",
       "2078  [@kg4vaal, lmaov, ., v, hard, the, ', ny, ', i...   \n",
       "6649  [rt, abbswinston, #zionist, #terrorist, demoli...   \n",
       "6251  [new, #photo, oak, in, a, snowstorm, http, //t...   \n",
       "\n",
       "                                             embeddings  \n",
       "3067                 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  \n",
       "3452       [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]  \n",
       "4536  [22, 20, 20, 20, 23, 24, 25, 26, 27, 28, 29, 3...  \n",
       "4941  [38, 39, 30, 16, 40, 41, 42, 43, 19, 20, 44, 3...  \n",
       "1295  [48, 49, 50, 51, 52, 53, 54, 30, 51, 52, 55, 5...  \n",
       "...                                                 ...  \n",
       "6705                                 [1416, 87, 87, 87]  \n",
       "5386  [2431, 4239, 617, 211, 30, 13081, 9137, 211, 8...  \n",
       "2078  [23534, 23535, 20, 6234, 2363, 126, 4, 7306, 4...  \n",
       "6649  [1208, 1471, 1472, 1473, 988, 23539, 8444, 161...  \n",
       "6251  [119, 5005, 5006, 53, 30, 2131, 43, 19, 20, 23...  \n",
       "\n",
       "[7613 rows x 7 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = GloVe(name='840B', dim=300)\n",
    "\n",
    "word_to_index = {}\n",
    "index = 0\n",
    "\n",
    "for df_index, row in df.iterrows():\n",
    "    for word in row[\"tokenized_text\"]:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = index\n",
    "            index += 1\n",
    "\n",
    "def embed(tokens):\n",
    "    numerical_sentence = [word_to_index[word] for word in tokens]\n",
    "    return numerical_sentence\n",
    "\n",
    "\n",
    "df[\"embeddings\"] = df[\"tokenized_text\"].apply(embed)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  119,  5005,  5006,    53,    30,  2131,    43,    19,    20, 23543,\n",
       "           15,   126,  5009,  5010,  5008,  5011,  5012,  5013, 23544,  5014])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_sentences = [torch.tensor(sentence) for sentence in df[\"embeddings\"].tolist()]\n",
    "labels = torch.tensor(df[\"target\"].tolist())\n",
    "\n",
    "numerical_sentences[len(numerical_sentences) -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sentences = pad_sequence(numerical_sentences, batch_first=True)\n",
    "\n",
    "embedding_train, embedding_validation = train_test_split(padded_sentences, test_size=0.1)\n",
    "label_train, label_validation = train_test_split(labels, test_size=0.1)\n",
    "\n",
    "assert len(embedding_train) == len(label_train)\n",
    "assert len(embedding_validation) == len(label_validation)\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 1000\n",
    "\n",
    "train_dataset = TensorDataset(embedding_train, label_train)\n",
    "val_dataset = TensorDataset(embedding_validation, label_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(dtype=model.parameters().__next__().dtype)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
