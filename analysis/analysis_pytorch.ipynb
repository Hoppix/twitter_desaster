{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a classifier pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDisasterClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
    "        super(TweetDisasterClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "        \n",
    "        self.fc_last = nn.Linear(hidden_sizes[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            out = hidden_layer(out)\n",
    "            out = self.relu(out)\n",
    "        \n",
    "        out = self.fc_last(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 30 * 300 # dimensionality for maximum of 30 words\n",
    "hidden_size = [128, 264, 128]\n",
    "num_classes = 2\n",
    "\n",
    "model = TweetDisasterClassifier(input_size, hidden_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "df = df.sample(frac=1)\n",
    "df['tokenized_text'] = df['text'].apply(tokenizer)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>1905</td>\n",
       "      <td>burning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@nagel_ashley @Vicken52 @BasedLaRock @goonc1ty...</td>\n",
       "      <td>0</td>\n",
       "      <td>[@nagel_ashley, @vicken52, @basedlarock, @goon...</td>\n",
       "      <td>[3000000, 3000000, 3000000, 3000000, 10252, 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2622</th>\n",
       "      <td>3763</td>\n",
       "      <td>destruction</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>REPUBLICAN STYLED ECONOMIC DESTRUCTION | Under...</td>\n",
       "      <td>0</td>\n",
       "      <td>[republican, styled, economic, destruction, |,...</td>\n",
       "      <td>[21414, 3000000, 20347, 3000000, 1423, 3000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6670</th>\n",
       "      <td>9559</td>\n",
       "      <td>thunder</td>\n",
       "      <td>Enfield, UK</td>\n",
       "      <td>Illusoria Icarus nowplaying check out http://t...</td>\n",
       "      <td>0</td>\n",
       "      <td>[illusoria, icarus, nowplaying, check, out, ht...</td>\n",
       "      <td>[3000000, 249002, 3000000, 790059, 3000000, 52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5116</th>\n",
       "      <td>7296</td>\n",
       "      <td>nuclear%20reactor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Check out this awesome profile on #GE's swimmi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[check, out, this, awesome, profile, on, #ge, ...</td>\n",
       "      <td>[528, 3000000, 56, 3000000, 27, 3000000, 1378,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4663</th>\n",
       "      <td>6629</td>\n",
       "      <td>inundated</td>\n",
       "      <td>United States</td>\n",
       "      <td>#tech Data Overload: The Growing Demand for Co...</td>\n",
       "      <td>0</td>\n",
       "      <td>[#tech, data, overload, the, growing, demand, ...</td>\n",
       "      <td>[272712, 3000000, 335, 3000000, 21856, 3000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7320</th>\n",
       "      <td>10479</td>\n",
       "      <td>wild%20fires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@EnzasBargains A5 Donated some fruit snacks &amp;a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[@enzasbargains, a5, donated, some, fruit, sna...</td>\n",
       "      <td>[3000000, 94242, 3000000, 9038, 3000000, 85, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>2211</td>\n",
       "      <td>chemical%20emergency</td>\n",
       "      <td>Virginia, United States</td>\n",
       "      <td>#Illinois: Emergency units simulate a chemical...</td>\n",
       "      <td>1</td>\n",
       "      <td>[#illinois, emergency, units, simulate, a, che...</td>\n",
       "      <td>[3000000, 3006, 3000000, 2312, 3000000, 24647,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119</th>\n",
       "      <td>4479</td>\n",
       "      <td>electrocuted</td>\n",
       "      <td>Redondo Beach, CA</td>\n",
       "      <td>Do babies actually get electrocuted from wall ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[do, babies, actually, get, electrocuted, from...</td>\n",
       "      <td>[47, 3000000, 5034, 3000000, 406, 3000000, 86,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>3111</td>\n",
       "      <td>debris</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>Malaysia confirms plane debris washed up on Re...</td>\n",
       "      <td>1</td>\n",
       "      <td>[malaysia, confirms, plane, debris, washed, up...</td>\n",
       "      <td>[36811, 3000000, 14125, 3000000, 3957, 3000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4392</th>\n",
       "      <td>6244</td>\n",
       "      <td>hijacking</td>\n",
       "      <td>Mongolia</td>\n",
       "      <td>#hot  Funtenna: hijacking computers to send da...</td>\n",
       "      <td>1</td>\n",
       "      <td>[#hot, funtenna, hijacking, computers, to, sen...</td>\n",
       "      <td>[162690, 3000000, 3000000, 52370, 3000000, 360...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id               keyword                 location  \\\n",
       "1318   1905               burning                      NaN   \n",
       "2622   3763           destruction              Houston, TX   \n",
       "6670   9559               thunder              Enfield, UK   \n",
       "5116   7296     nuclear%20reactor                      NaN   \n",
       "4663   6629             inundated            United States   \n",
       "...     ...                   ...                      ...   \n",
       "7320  10479          wild%20fires                      NaN   \n",
       "1529   2211  chemical%20emergency  Virginia, United States   \n",
       "3119   4479          electrocuted        Redondo Beach, CA   \n",
       "2169   3111                debris                 Nigeria    \n",
       "4392   6244             hijacking                 Mongolia   \n",
       "\n",
       "                                                   text  target  \\\n",
       "1318  @nagel_ashley @Vicken52 @BasedLaRock @goonc1ty...       0   \n",
       "2622  REPUBLICAN STYLED ECONOMIC DESTRUCTION | Under...       0   \n",
       "6670  Illusoria Icarus nowplaying check out http://t...       0   \n",
       "5116  Check out this awesome profile on #GE's swimmi...       0   \n",
       "4663  #tech Data Overload: The Growing Demand for Co...       0   \n",
       "...                                                 ...     ...   \n",
       "7320  @EnzasBargains A5 Donated some fruit snacks &a...       1   \n",
       "1529  #Illinois: Emergency units simulate a chemical...       1   \n",
       "3119  Do babies actually get electrocuted from wall ...       1   \n",
       "2169  Malaysia confirms plane debris washed up on Re...       1   \n",
       "4392  #hot  Funtenna: hijacking computers to send da...       1   \n",
       "\n",
       "                                         tokenized_text  \\\n",
       "1318  [@nagel_ashley, @vicken52, @basedlarock, @goon...   \n",
       "2622  [republican, styled, economic, destruction, |,...   \n",
       "6670  [illusoria, icarus, nowplaying, check, out, ht...   \n",
       "5116  [check, out, this, awesome, profile, on, #ge, ...   \n",
       "4663  [#tech, data, overload, the, growing, demand, ...   \n",
       "...                                                 ...   \n",
       "7320  [@enzasbargains, a5, donated, some, fruit, sna...   \n",
       "1529  [#illinois, emergency, units, simulate, a, che...   \n",
       "3119  [do, babies, actually, get, electrocuted, from...   \n",
       "2169  [malaysia, confirms, plane, debris, washed, up...   \n",
       "4392  [#hot, funtenna, hijacking, computers, to, sen...   \n",
       "\n",
       "                                             embeddings  \n",
       "1318  [3000000, 3000000, 3000000, 3000000, 10252, 30...  \n",
       "2622  [21414, 3000000, 20347, 3000000, 1423, 3000000...  \n",
       "6670  [3000000, 249002, 3000000, 790059, 3000000, 52...  \n",
       "5116  [528, 3000000, 56, 3000000, 27, 3000000, 1378,...  \n",
       "4663  [272712, 3000000, 335, 3000000, 21856, 3000000...  \n",
       "...                                                 ...  \n",
       "7320  [3000000, 94242, 3000000, 9038, 3000000, 85, 3...  \n",
       "1529  [3000000, 3006, 3000000, 2312, 3000000, 24647,...  \n",
       "3119  [47, 3000000, 5034, 3000000, 406, 3000000, 86,...  \n",
       "2169  [36811, 3000000, 14125, 3000000, 3957, 3000000...  \n",
       "4392  [162690, 3000000, 3000000, 52370, 3000000, 360...  \n",
       "\n",
       "[7613 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = GloVe(name='840B', dim=300)\n",
    "\n",
    "def embed(tokens):\n",
    "    embeds = []\n",
    "    for token in tokens:\n",
    "        if token in glove.stoi:\n",
    "            embeds.append(glove.stoi[token])\n",
    "        embeds.append(3000000) # maybe change this we ignore alot of unknowns here\n",
    "    \n",
    "    return embeds\n",
    "\n",
    "\n",
    "df[\"embeddings\"] = df[\"tokenized_text\"].apply(embed)\n",
    "df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Convert the data into PyTorch tensors\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(X_train)\n\u001b[1;32m      6\u001b[0m y_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(y_train)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Create a data loader\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "# Create a data loader\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
